{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyN2Md04G4sDBrNLthuQQxPh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Train Liveness check model"],"metadata":{"id":"CTa9ygZXbDTA"}},{"cell_type":"markdown","source":["The original CASIA-FASD is not readily availabe, so I use smaller subset from Kaggle for this demo"],"metadata":{"id":"ygUJUj8LbQoI"}},{"cell_type":"code","source":["import gdown\n","import zipfile\n","import os\n","import numpy as np\n","from PIL import Image\n","\n","# Download dataset (Casia FASD)\n","file_id = '1nXoBxXhfGs5IiSojc8ZUHnlesxgXzcg7'\n","# file_id = '1m9dVjwU-KajHbuUMVHgENg62fUdCZg1f'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","\n","zip_file_path = '/content/casia_fasd.zip'\n","gdown.download(url, zip_file_path)\n","\n","extract_folder_path = '/content/casia_fasd'\n","os.makedirs(extract_folder_path, exist_ok=True)\n","\n","# Extract the ZIP file\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_folder_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bYgjjyY3fD9","executionInfo":{"status":"ok","timestamp":1725943095784,"user_tz":-480,"elapsed":13461,"user":{"displayName":"Ahmad Sena","userId":"00694891711741380336"}},"outputId":"6c8b1d99-74ab-4414-e1bf-79c9654bcca3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1nXoBxXhfGs5IiSojc8ZUHnlesxgXzcg7\n","From (redirected): https://drive.google.com/uc?id=1nXoBxXhfGs5IiSojc8ZUHnlesxgXzcg7&confirm=t&uuid=fd16ad7b-9c0a-4d12-893d-e3e596b31752\n","To: /content/casia_fasd.zip\n","100%|██████████| 74.0M/74.0M [00:02<00:00, 25.8MB/s]\n"]}]},{"cell_type":"code","source":["def calculate_normalization(folder_path):\n","    def load_images_from_folder(folder_path):\n","        images = []\n","        for filename in os.listdir(folder_path):\n","            img_path = os.path.join(folder_path, filename)\n","            try:\n","                with Image.open(img_path) as img:\n","                    img = img.convert('RGB')\n","                    images.append(np.array(img))\n","            except IOError:\n","                print(f\"Error loading image: {img_path}\")\n","        return images\n","\n","    def compute_normalization_values(images):\n","        images_array = np.stack(images, axis=0)\n","        mean = np.mean(images_array, axis=(0, 1, 2))\n","        std = np.std(images_array, axis=(0, 1, 2))\n","        return mean, std\n","\n","    images = load_images_from_folder(folder_path)\n","\n","    mean, std = compute_normalization_values(images)\n","\n","    return mean,std\n","\n","# only use training data for calculating normalization value\n","folder_path = '/content/casia_fasd/train_img/train_img/color/'\n","normalization_values = calculate_normalization(folder_path)\n","normalization_values\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Tkaw9hHm-qS","executionInfo":{"status":"ok","timestamp":1725943117254,"user_tz":-480,"elapsed":21491,"user":{"displayName":"Ahmad Sena","userId":"00694891711741380336"}},"outputId":"1346bf62-10c2-432e-e50a-acd72223bff9"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([105.32685981,  91.9504575 ,  91.54538125]),\n"," array([63.06708699, 58.47346108, 58.98859229]))"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n","from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score\n","\n","\n","# Define the dataset class\n","class ImageDataset(Dataset):\n","    def __init__(self, root_dir, split='train', transform=None):\n","        self.root_dir = os.path.join(root_dir, f'{split}_img/{split}_img/color')\n","        self.transform = transform\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for file_name in os.listdir(self.root_dir):\n","            if file_name.endswith('.jpg'):\n","                label = 'real' if 'real' in file_name else 'fake'\n","                self.image_paths.append(os.path.join(self.root_dir, file_name))\n","                self.labels.append(0 if label == 'fake' else 1)  # Encode labels as 0 and 1\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.labels[idx]\n","        label = torch.tensor(label, dtype=torch.float)  # Ensure labels are float\n","        return image, label\n","\n","\n","# Define data transformations\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=normalization_values[0], std=normalization_values[1]),\n","])\n","\n","val_test_transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=normalization_values[0], std=normalization_values[1]),\n","])\n","\n","# Load datasets\n","full_train_dataset = ImageDataset(root_dir='/content/casia_fasd', split='train', transform=train_transform)\n","\n","train_dataset, val_dataset = random_split(full_train_dataset, [0.8, 0.2])\n","val_dataset.dataset.transform = val_test_transform  # Change to transform func without augmentation\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n","test_dataset = ImageDataset(root_dir='/content/casia_fasd', split='test', transform=val_test_transform)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n","\n","\n","# Define the model\n","class SpoofNet(nn.Module):\n","    def __init__(self):\n","        super(SpoofNet, self).__init__()\n","        # Load MobileNetV2 with pretrained weights\n","        self.mobilenet = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n","        self.features = self.mobilenet.features\n","\n","        # Adding extra layers\n","        self.conv2d = nn.Conv2d(1280, 32, kernel_size=(3, 3), padding=1)\n","        self.relu = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.2)\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(32, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.conv2d(x)\n","        x = self.relu(x)\n","        x = self.dropout1(x)\n","        x = self.global_avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","\n","# Initialize and move the model to the GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = SpoofNet().to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()  # For binary classification\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","threshold = 0.5  # Apply threshold for binary classification\n","\n","\n","# Training function\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, threshold=0.5):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)  # Ensure labels are float\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs.squeeze(), labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item() * images.size(0)\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        all_labels = []\n","        all_preds = []\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs.squeeze(), labels)\n","                val_loss += loss.item() * inputs.size(0)\n","                preds = (outputs.squeeze() > threshold)\n","                all_labels.extend(labels.cpu().numpy())\n","                all_preds.extend(preds.cpu().numpy())\n","\n","        val_loss /= len(val_loader.dataset)\n","        val_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n","        val_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n","        val_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n","        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, F1 Score: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n","\n","\n","# Train the model\n","train_model(model, train_loader, val_loader, criterion, optimizer, 3, threshold)\n","\n","\n","def test_model(model, test_loader, threshold=0.5):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            preds = (outputs.squeeze() > threshold)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(preds.cpu().numpy())\n","\n","    return y_true, y_pred\n","\n","\n","# Evaluate the model\n","y_true, y_pred = test_model(model, test_loader, threshold)\n","\n","# Calculate additional metrics\n","accuracy = accuracy_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n","recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n","precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n","\n","# Print metrics and classification report\n","print(f'Accuracy: {accuracy:.4f}')\n","print(f'F1 Score: {f1:.4f}')\n","print(f'Recall: {recall:.4f}')\n","print(f'Precision: {precision:.4f}')\n","print('Classification Report:')\n","print(classification_report(y_true, y_pred, target_names=['Fake', 'Real']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYWfBcxfIBzS","executionInfo":{"status":"ok","timestamp":1725944219102,"user_tz":-480,"elapsed":765868,"user":{"displayName":"Ahmad Sena","userId":"00694891711741380336"}},"outputId":"446c2cf0-3257-4e23-8acb-a051295a4248"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Loss: 0.2085, Validation Loss: 0.6382, F1 Score: 0.6623, Recall: 0.7644, Precision: 0.5842\n","Epoch 2/3, Loss: 0.0388, Validation Loss: 1.0236, F1 Score: 0.6694, Recall: 0.7674, Precision: 0.8217\n","Epoch 3/3, Loss: 0.0282, Validation Loss: 0.8332, F1 Score: 0.6731, Recall: 0.7644, Precision: 0.7045\n","Accuracy: 0.7608\n","F1 Score: 0.6698\n","Recall: 0.7608\n","Precision: 0.7529\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        Fake       0.76      1.00      0.86      1817\n","        Real       0.73      0.04      0.08       591\n","\n","    accuracy                           0.76      2408\n","   macro avg       0.74      0.52      0.47      2408\n","weighted avg       0.75      0.76      0.67      2408\n","\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'spoof_weight.pth')"],"metadata":{"id":"i90js3vyielN","executionInfo":{"status":"ok","timestamp":1725944219102,"user_tz":-480,"elapsed":3,"user":{"displayName":"Ahmad Sena","userId":"00694891711741380336"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Train Face recognition model"],"metadata":{"id":"_bTt1XmcidzY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n","from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score\n","from sklearn.datasets import fetch_lfw_people\n","\n","# Load the LFW dataset\n","lfw_dataset = fetch_lfw_people(min_faces_per_person=70, color=True)\n","X = lfw_dataset.images\n","y = lfw_dataset.target\n","target_names = lfw_dataset.target_names\n","\n","\n","# Preprocess the images\n","class LFWDataset(Dataset):\n","    def __init__(self, images, labels, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create the full dataset\n","full_dataset = LFWDataset(images=X, labels=y, transform=transform)\n","\n","\n","# Use random_split to split the dataset into train, val, and test\n","train_dataset, val_dataset, test_dataset = random_split(full_dataset, [0.72, 0.08, 0.2])\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","\n","# Define the model\n","class FaceRecogNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(FaceRecogNet, self).__init__()\n","        # Load MobileNetV2 with the weights argument\n","        self.mobilenet = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n","        self.mobilenet.classifier = nn.Sequential(\n","            nn.Linear(self.mobilenet.last_channel, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.mobilenet(x)\n","\n","\n","num_classes = len(target_names)\n","\n","# Initialize and move the model to the GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = FaceRecogNet(num_classes=num_classes).to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","# Training function\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item() * inputs.size(0)\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        all_labels = []\n","        all_preds = []\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                all_labels.extend(labels.cpu().numpy())\n","                all_preds.extend(preds.cpu().numpy())\n","\n","        val_loss /= len(val_loader.dataset)\n","        val_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n","        val_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n","        val_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n","        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, F1 Score: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n","\n","\n","# Train the model\n","train_model(model, train_loader, val_loader, criterion, optimizer, 3)\n","\n","\n","# Evaluation function\n","def test_model(model, test_loader):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(preds.cpu().numpy())\n","    return y_true, y_pred\n","\n","\n","# Evaluate the model\n","y_true, y_pred = test_model(model, test_loader)\n","\n","# Calculate additional metrics\n","accuracy = accuracy_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n","recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n","precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n","\n","# Print metrics and classification report\n","print(f'Accuracy: {accuracy:.4f}')\n","print(f'F1 Score: {f1:.4f}')\n","print(f'Recall: {recall:.4f}')\n","print(f'Precision: {precision:.4f}')\n","print('Classification Report:')\n","print(classification_report(y_true, y_pred, target_names=target_names))\n"],"metadata":{"id":"28aFRA1yRoB4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725944754918,"user_tz":-480,"elapsed":535818,"user":{"displayName":"Ahmad Sena","userId":"00694891711741380336"}},"outputId":"9613974f-5b82-4f79-f6b9-b9ad9bd1c50a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Loss: 1.6902, Validation Loss: 1.4725, F1 Score: 0.2963, Recall: 0.4660, Precision: 0.2172\n","Epoch 2/3, Loss: 1.3060, Validation Loss: 1.0471, F1 Score: 0.5277, Recall: 0.6602, Precision: 0.4417\n","Epoch 3/3, Loss: 0.9478, Validation Loss: 0.7602, F1 Score: 0.6279, Recall: 0.7087, Precision: 0.6208\n","Accuracy: 0.6887\n","F1 Score: 0.6095\n","Recall: 0.6887\n","Precision: 0.6463\n","Classification Report:\n","                   precision    recall  f1-score   support\n","\n","     Ariel Sharon       1.00      0.06      0.11        18\n","     Colin Powell       0.49      1.00      0.66        40\n","  Donald Rumsfeld       0.92      0.41      0.56        27\n","    George W Bush       0.80      0.99      0.89       117\n","Gerhard Schroeder       0.00      0.00      0.00        24\n","      Hugo Chavez       0.00      0.00      0.00        11\n","       Tony Blair       0.50      0.45      0.47        20\n","\n","         accuracy                           0.69       257\n","        macro avg       0.53      0.41      0.38       257\n","     weighted avg       0.65      0.69      0.61       257\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'face_recognition_weight.pth')"],"metadata":{"id":"JMpt73-BNoqi","executionInfo":{"status":"ok","timestamp":1725944754918,"user_tz":-480,"elapsed":5,"user":{"displayName":"Ahmad Sena","userId":"00694891711741380336"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import json\n","with open('target_names.json','w') as f:\n","  json.dump(list(target_names),f)"],"metadata":{"id":"NLfLdNo4uv_J","executionInfo":{"status":"ok","timestamp":1725944838503,"user_tz":-480,"elapsed":377,"user":{"displayName":"Ahmad Sena","userId":"00694891711741380336"}}},"execution_count":10,"outputs":[]}]}